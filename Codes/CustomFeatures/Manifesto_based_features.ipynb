{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compute the popularity of policy issues based on recent manifestos. \n",
    "We build a policy category classifier (8-classes) with all manifestos written in English.\n",
    "Then we keep a popularity index based on the target country's recent election manifestos. For a petition, we classify the policy category of each of the sentences and aggregate the popularity score at the petition-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#We clean the manifestos from manifesto project https://manifesto-project.wzb.eu\n",
    "\n",
    "def clean_manifesto(text, label): \n",
    "    sentence = [] \n",
    "    cmpcode = [] \n",
    "    for i in xrange(0, len(text)): \n",
    "        if len(str(text[i]))>3 and str(text[i])!=np.nan: \n",
    "            line = text[i].lower().split() \n",
    "            words = [w for w in line] \n",
    "            sentence.append(\" \".join(words))            \n",
    "            if (label[i]=='NA') or (np.isnan(label[i])): \n",
    "                cmpcode.append(0) \n",
    "            else: \n",
    "                cmpcode.append(int(label[i]/100))\n",
    "    return sentence, cmpcode\n",
    "\n",
    "import os, pandas as pd, numpy as np \n",
    "cmplabelsc = [] \n",
    "\n",
    "for i in os.listdir('EnglishManifestos/'): \n",
    "    x = pd.read_csv(i, header=0) \n",
    "    sentence, cmpcode = clean_manifesto(x.text, x.cmp_code) \n",
    "    cmplabelsc.extend(cmpcode)\n",
    "                \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_features = 8000, ngram_range=(1,1), stop_words='english')\n",
    "sent_vec = tf_vectorizer.fit_transform(np.array(sentences))\n",
    "sent_vect = np.array(sent_vec.toarray())\n",
    "labelsc = np.array(cmplabelsc).reshape((len(cmplabelsc),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Popularity score for each policy category 0 to 7\n",
    "# Based on recent manifestos of the country\n",
    "\n",
    "popularitysc = []\n",
    "for i in os.listdir('RecentManifestos/'): \n",
    "    x = pd.read_csv(i, header=0) \n",
    "    sentence, cmpcode = clean_manifesto(x.text, x.cmp_code) \n",
    "    popularitysc.extend(cmpcode)\n",
    "    \n",
    "import collections\n",
    "popularity = []\n",
    "c = collections.Counter(popularitysc)\n",
    "for i,j in c.iteritems():\n",
    "    popularity.append((j*1.0)/len(popularitysc))\n",
    "    \n",
    "def popularitys(indx):\n",
    "    return popularity[indx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN model for classifying sentences into one of 7 policy categories\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras.models import Model\n",
    "\n",
    "modelc = Sequential() \n",
    "modelc.add(Dense(300, input_dim=8000, activation='relu')) \n",
    "modelc.add(Dense(8, activation='softmax', name='main_output'))\n",
    "\n",
    "batch_size = 128 \n",
    "nb_epoch = 3\n",
    "\n",
    "Y = [0,1,2,3,4,5,6,7]\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "\n",
    "encoded_Y = encoder.transform(labelsc)\n",
    "sentlabelsc = np_utils.to_categorical(encoded_Y, num_classes=8)\n",
    "\n",
    "modelc.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc']) \n",
    "\n",
    "modelc.fit(sent_vect, sentlabelsc, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute sentence level policy area, and aggregate popularity based on recent election manifestos of the target country\n",
    "import pandas as pd\n",
    "petitions = pd.read_csv(\"WTP_dataset_1K.csv\", header=0)\n",
    "petitions['signs'] = petitions['signs'].astype('int')\n",
    "petitions['fulltext'] = petitions['title'].astype('str')+' '+petitions['content'].astype('str')\n",
    "\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "cdata = []\n",
    "for i in petitions['fulltext']:\n",
    "    count = 0\n",
    "    x = tf_vectorizer.transform(sent_tokenize(i.decode('utf-8','ignore')))\n",
    "    lab = modelc.predict(x.toarray(), verbose=0)\n",
    "    for j in lab:\n",
    "        if np.argmax(j) > 0:\n",
    "            count += popularitys(np.argmax(j))\n",
    "    cdata.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute political bias and left-right scale. We build a 3-class NN classifier for sentences using manifesto text. Mapping is taken from Volkens et al., 2013 work. For a petition, each sentence is classified into one of 3 classes, and political bias and left-right scale are computed by aggregating them at petition-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Political bias and Left-right score \n",
    "# Here we do a similar manifesto cleaning, except we map classes to left/right/neutral classes\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def getleftright(labels):\n",
    "    newlabels = []\n",
    "    left = [103, 105, 106, 107, 202, 403, 404, 406, 412, 413, 504, 506, 701]\n",
    "    right = [104, 201, 203, 305, 401, 402, 407, 414, 505, 601, 603, 605, 606]\n",
    "\n",
    "    for i in labels:\n",
    "        if i in left:\n",
    "                newlabels.append(1)\n",
    "        elif i in right:\n",
    "                newlabels.append(2)\n",
    "        else:\n",
    "                newlabels.append(0)\n",
    "\n",
    "    return newlabels\n",
    "\n",
    "def clean_manifesto(text, label): \n",
    "    sentence = [] \n",
    "    cmpcode = [] \n",
    "    for i in xrange(0, len(text)): \n",
    "        if len(str(text[i]))>3 and str(text[i])!=np.nan: \n",
    "            line = text[i].lower().split() \n",
    "            words = [w for w in line] \n",
    "            sentence.append(\" \".join(words))\n",
    "            \n",
    "            if (label[i]=='NA') or (np.isnan(label[i])): \n",
    "                cmpcode.append(0) \n",
    "            else: \n",
    "                cmpcode.append(int(label[i]))\n",
    "    labels = getleftright(cmpcode)\n",
    "    return sentence, labels\n",
    "\n",
    "import os, pandas as pd, numpy as np \n",
    "\n",
    "sentences = [] \n",
    "cmplabels = [] \n",
    "\n",
    "for i in os.listdir('EnglishManifestos/'): \n",
    "    x = pd.read_csv(i, header=0) \n",
    "    sentence, cmpcode = clean_manifesto(x.text, x.cmp_code) \n",
    "    sentences.extend(sentence) \n",
    "    cmplabels.extend(cmpcode)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_features = 8000, ngram_range=(1,1), stop_words='english')\n",
    "sent_vec = tf_vectorizer.fit_transform(np.array(sentences))\n",
    "labels = np.array(cmplabels).reshape((len(cmplabels),1))\n",
    "sent_vect = np.array(sent_vec.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras.models import Model\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(300, input_dim=8000, activation='relu')) \n",
    "model.add(Dense(3, activation='softmax', name='main_output'))\n",
    "\n",
    "batch_size = 128 \n",
    "nb_epoch = 3\n",
    "\n",
    "Y = [0,1,2]\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "\n",
    "encoded_Y = encoder.transform(labels)\n",
    "sentlabels = np_utils.to_categorical(encoded_Y, num_classes=3)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc']) \n",
    "\n",
    "model.fit(sent_vect, sentlabels, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "pbias = []\n",
    "lrscale = []\n",
    "for i in petitions['fulltext']:\n",
    "    count = [0,0,0]\n",
    "    x = tf_vectorizer.transform(sent_tokenize(i.decode('utf-8','ignore')))\n",
    "    lab = model.predict(x.toarray(), verbose=0)\n",
    "    for j in lab:\n",
    "        count = count + j\n",
    "    lr_ratio = (count[1] - count[2])/(count[1]+count[2])\n",
    "    bias = (count[1] + count[2])/(count[1]+count[2]+count[0])\n",
    "    lrscale.append(lr_ratio)\n",
    "    pbias.append(bias)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
